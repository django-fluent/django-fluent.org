apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: {{ template "fullname" . }}
  labels:
    app: {{ template "name" . }}
    chart: {{ .Chart.Name }}-{{ .Chart.Version | replace "+" "_" }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
spec:
  replicas: {{ .Values.replicaCount }}
  revisionHistoryLimit: 0  # disables deployment rollback, but using helm for that anyway
  {{- if eq .Values.replicaCount 1.0 }}
  strategy:
    rollingUpdate:
      maxUnavailable: 0  # Avoid Terminating and ContainerCreating at the same time
  {{- end }}
  selector:
    matchLabels:
      app: {{ template "name" . }}
      release: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app: {{ template "name" . }}
        release: {{ .Release.Name }}
    spec:
      {{- with .Values.image.pullSecrets }}
      imagePullSecrets:
{{ toYaml . | indent 8 }}
      {{- end }}
      initContainers:
        - name: {{ .Chart.Name }}-migrate
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          command: ['/bin/sh', '-c', 'manage.py check --deploy && manage.py migrate']
          {{- if .Values.envSecret }}
          envFrom:
            - secretRef:
                name: {{ .Values.envSecret }}
          {{- end }}
          {{- if .Values.env }}
          env:
            {{- range $key, $value := .Values.env }}
            - name: {{ $key | quote }}
              value: {{ $value | quote }}
            {{- end }}
          {{- end }}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop: ['all']
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 8080
            - name: stats
              containerPort: 1717
          {{- if .Values.envSecret }}
          envFrom:
            - secretRef:
                name: {{ .Values.envSecret }}
          {{- end }}
          {{- if .Values.env }}
          env:
            {{- range $key, $value := .Values.env }}
            - name: {{ $key | quote }}
              value: {{ $value | quote }}
            {{- end }}
          {{- end }}
          volumeMounts:
            - name: media
              mountPath: /app/web/media
          {{- if .Values.livenessProbe.enabled }}
          livenessProbe:
            initialDelaySeconds: {{ .Values.livenessProbe.initialDelaySeconds }}
            periodSeconds: {{ .Values.livenessProbe.periodSeconds }}
            timeoutSeconds: {{ .Values.livenessProbe.timeoutSeconds }}
            successThreshold: {{ .Values.livenessProbe.successThreshold }}
            failureThreshold: {{ .Values.livenessProbe.failureThreshold }}
            httpGet:
              path: {{ .Values.livenessProbe.url }}
              port: 8080
              httpHeaders:
                - name: Host
                  value: localhost
          {{- end }}
          {{- if .Values.readinessProbe.enabled }}
          readinessProbe:
            initialDelaySeconds: {{ .Values.readinessProbe.initialDelaySeconds }}
            periodSeconds: {{ .Values.readinessProbe.periodSeconds }}
            timeoutSeconds: {{ .Values.readinessProbe.timeoutSeconds }}
            successThreshold: {{ .Values.readinessProbe.successThreshold }}
            failureThreshold: {{ .Values.readinessProbe.failureThreshold }}
            httpGet:
              path: {{ .Values.readinessProbe.url }}
              port: 8080
              httpHeaders:
                - name: Host
                  value: localhost
          {{- end }}
          lifecycle:
            preStop:
              exec:
                # https://github.com/kubernetes/contrib/issues/1140
                # This hack avoids 0% downtime during deploys.
                # After the preStop, containers receive a SIGTERM meaning:
                # "cleanup what you're doing and stop accepting new work".
                # Yet traffic may still briefly be sent to it by the ingress
                # controller. This hack delays that SIGTERM moment so the new
                # container is already actively used to handle traffic.
                command: ["sleep", "15"]
          resources:
{{ toYaml .Values.resources | indent 12 }}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop: ['all']
    {{- with .Values.securityContext }}
      securityContext:
{{ toYaml . | indent 8 }}
    {{- end }}
    {{- with .Values.nodeSelector }}
      nodeSelector:
{{ toYaml . | indent 8 }}
    {{- end }}
    {{- with .Values.affinity }}
      affinity:
{{ toYaml . | indent 8 }}
    {{- end }}
    {{- with .Values.tolerations }}
      tolerations:
{{ toYaml . | indent 8 }}
    {{- end }}
      volumes:
        - name: media
        {{- if .Values.persistence.enabled }}
          persistentVolumeClaim:
            claimName: {{ .Values.persistence.media.existingClaim | default (printf "%s-media" (include "fullname" .)) }}
        {{- else }}
          emptyDir: {}
        {{- end }}
